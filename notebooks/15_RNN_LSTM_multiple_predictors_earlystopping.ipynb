{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Long Short Term Memory Multple Predictors\n",
    "\n",
    "Stockhold dataset: [br.financas.yahoo.com/quote/PTR4.SA](https://br.financas.yahoo.com/quote/PBR/history/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '2.2.2', '3.9.2', '2.17.0', '1.5.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, pd.__version__, matplotlib.__version__, tf.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "...          ...  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1242 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/petr/petr4_treinamento.csv')\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9990000e+01, 2.0209999e+01, 1.9690001e+01, 1.9690001e+01,\n",
       "        1.8086271e+01, 3.0182600e+07],\n",
       "       [1.9809999e+01, 2.0400000e+01, 1.9700001e+01, 2.0400000e+01,\n",
       "        1.8738441e+01, 3.0552600e+07],\n",
       "       [2.0330000e+01, 2.0620001e+01, 2.0170000e+01, 2.0430000e+01,\n",
       "        1.8766001e+01, 3.6141000e+07],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data.iloc[:, 1:7].values\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_train_data = normalizer.fit_transform(train_data)\n",
    "normalized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938],\n",
       "       [0.7562984 ],\n",
       "       [0.78149225],\n",
       "       ...,\n",
       "       [0.57122093],\n",
       "       [0.57655039],\n",
       "       [0.57655039]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer_prediction = MinMaxScaler(feature_range=(0,1))\n",
    "normalizer_prediction.fit_transform(train_data[:,0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "       0.0437121 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       [0.78875969, 0.7949588 , 0.81013311, 0.77996075, 0.70144373,\n",
       "        0.04015963],\n",
       "       [0.77083338, 0.77363063, 0.78505624, 0.75147351, 0.67522435,\n",
       "        0.0416214 ],\n",
       "       [0.74806197, 0.75618037, 0.78505624, 0.76031438, 0.68336137,\n",
       "        0.03485382],\n",
       "       [0.75436047, 0.76490543, 0.78915051, 0.76768177, 0.69014234,\n",
       "        0.02507502],\n",
       "       [0.75823643, 0.76442079, 0.79733884, 0.77013751, 0.6924025 ,\n",
       "        0.0260728 ],\n",
       "       [0.76598837, 0.77411537, 0.79682707, 0.76227897, 0.68516964,\n",
       "        0.0404927 ],\n",
       "       [0.76598837, 0.77411537, 0.79682707, 0.76719061, 0.68969016,\n",
       "        0.0423977 ],\n",
       "       [0.76017437, 0.75714973, 0.79222108, 0.76817293, 0.69059437,\n",
       "        0.02401858],\n",
       "       [0.75872098, 0.75908871, 0.79222108, 0.76178781, 0.68471746,\n",
       "        0.02821315],\n",
       "       [0.75581391, 0.75714973, 0.78915051, 0.75540279, 0.6788408 ,\n",
       "        0.02706042],\n",
       "       [0.74467054, 0.74309258, 0.77533265, 0.74607071, 0.67025175,\n",
       "        0.02587622],\n",
       "       [0.7374031 , 0.74357736, 0.77328557, 0.75540279, 0.6788408 ,\n",
       "        0.03367205],\n",
       "       [0.7374031 , 0.74454673, 0.77328557, 0.75392926, 0.67748471,\n",
       "        0.02460946],\n",
       "       [0.73498067, 0.75036355, 0.78045041, 0.75687631, 0.68019705,\n",
       "        0.02806007],\n",
       "       [0.75242248, 0.75327189, 0.77533265, 0.74508849, 0.66934774,\n",
       "        0.02878973],\n",
       "       [0.73401163, 0.73194382, 0.75332651, 0.73231836, 0.65759427,\n",
       "        0.03876941],\n",
       "       [0.71656977, 0.71352399, 0.71903787, 0.68762287, 0.6164569 ,\n",
       "        0.09583767],\n",
       "       [0.68120155, 0.68153175, 0.70522006, 0.68172891, 0.61103237,\n",
       "        0.04756616],\n",
       "       [0.67538755, 0.69704314, 0.71647907, 0.70039291, 0.62821037,\n",
       "        0.04129104],\n",
       "       [0.67635659, 0.68250121, 0.70470824, 0.67779964, 0.60741587,\n",
       "        0.04620398],\n",
       "       [0.63372098, 0.67959287, 0.67246673, 0.68172891, 0.61103237,\n",
       "        0.11064144],\n",
       "       [0.66521318, 0.66553563, 0.6862846 , 0.65815327, 0.58933361,\n",
       "        0.04418925],\n",
       "       [0.65649225, 0.66456617, 0.67553736, 0.65324168, 0.584813  ,\n",
       "        0.0530315 ],\n",
       "       [0.64680228, 0.65487159, 0.67860793, 0.6650295 , 0.5956623 ,\n",
       "        0.04444964],\n",
       "       [0.66618222, 0.66553563, 0.69651996, 0.66797641, 0.59837464,\n",
       "        0.03194532],\n",
       "       [0.65843028, 0.66068832, 0.6888434 , 0.66159139, 0.59249793,\n",
       "        0.0370597 ],\n",
       "       [0.64970935, 0.65535633, 0.6862846 , 0.6596267 , 0.59068976,\n",
       "        0.0357702 ],\n",
       "       [0.65116274, 0.66311202, 0.68577277, 0.67288805, 0.60289526,\n",
       "        0.02903152],\n",
       "       [0.66424419, 0.67426079, 0.70470824, 0.68271123, 0.61193639,\n",
       "        0.0412361 ],\n",
       "       [0.67344961, 0.67038294, 0.68730803, 0.65913564, 0.59023768,\n",
       "        0.03711206],\n",
       "       [0.64292631, 0.6446922 , 0.66939616, 0.64440082, 0.57667593,\n",
       "        0.04346845],\n",
       "       [0.64486434, 0.64178381, 0.65967247, 0.63605111, 0.56899095,\n",
       "        0.04421171],\n",
       "       [0.62257747, 0.62190984, 0.65148414, 0.62622798, 0.55994986,\n",
       "        0.04364257],\n",
       "       [0.60949617, 0.61027635, 0.63510752, 0.61591359, 0.55045665,\n",
       "        0.04779322],\n",
       "       [0.60998067, 0.61609307, 0.6407369 , 0.61935165, 0.55362107,\n",
       "        0.04092922],\n",
       "       [0.60852713, 0.60979157, 0.63613096, 0.60952857, 0.54457989,\n",
       "        0.03981569],\n",
       "       [0.59593023, 0.61803199, 0.62845445, 0.62377213, 0.55768961,\n",
       "        0.04509603],\n",
       "       [0.61143411, 0.62190984, 0.63254862, 0.60412577, 0.5396073 ,\n",
       "        0.05085238],\n",
       "       [0.60222863, 0.60542899, 0.6320368 , 0.60707267, 0.54231954,\n",
       "        0.04531064],\n",
       "       [0.64922481, 0.67862336, 0.6704196 , 0.68025539, 0.60967603,\n",
       "        0.10572707],\n",
       "       [0.68362398, 0.74212312, 0.72620261, 0.72445981, 0.65036132,\n",
       "        0.08930445],\n",
       "       [0.70687989, 0.72952012, 0.7185261 , 0.69597258, 0.62414194,\n",
       "        0.04376518],\n",
       "       [0.68265509, 0.71255448, 0.7062436 , 0.72347744, 0.64945705,\n",
       "        0.03589495],\n",
       "       [0.70978682, 0.72079491, 0.74257927, 0.71414542, 0.64086801,\n",
       "        0.03739277],\n",
       "       [0.70784879, 0.72370339, 0.74769703, 0.71463658, 0.64132019,\n",
       "        0.04530406],\n",
       "       [0.71608527, 0.73242845, 0.74104401, 0.74115922, 0.66573124,\n",
       "        0.03887614],\n",
       "       [0.73643411, 0.74066888, 0.76202661, 0.73133599, 0.65669001,\n",
       "        0.06269313],\n",
       "       [0.7122093 , 0.73097431, 0.75332651, 0.73673879, 0.6616627 ,\n",
       "        0.05787405],\n",
       "       [0.7122093 , 0.73097431, 0.75281474, 0.73182715, 0.65714209,\n",
       "        0.04839097],\n",
       "       [0.7194767 , 0.72176442, 0.74513818, 0.71954817, 0.6458407 ,\n",
       "        0.03954013],\n",
       "       [0.70348832, 0.70722254, 0.73541453, 0.70383112, 0.63137489,\n",
       "        0.03144514],\n",
       "       [0.69525189, 0.69995148, 0.73387917, 0.70874262, 0.63589531,\n",
       "        0.02308847],\n",
       "       [0.70397287, 0.70528357, 0.73183214, 0.70677803, 0.63408723,\n",
       "        0.03482392],\n",
       "       [0.70397287, 0.7081919 , 0.73490276, 0.70677803, 0.63408723,\n",
       "        0.02257928],\n",
       "       [0.69767442, 0.69510427, 0.72824974, 0.69842833, 0.6264022 ,\n",
       "        0.01903582],\n",
       "       [0.68168605, 0.68395536, 0.71136131, 0.67927317, 0.60877212,\n",
       "        0.02224034],\n",
       "       [0.68168605, 0.68395536, 0.69344933, 0.66306491, 0.59385423,\n",
       "        0.02942397],\n",
       "       [0.65310078, 0.66650509, 0.69396111, 0.67779964, 0.60741587,\n",
       "        0.02244093],\n",
       "       [0.66618222, 0.67571493, 0.6949847 , 0.66355598, 0.59430621,\n",
       "        0.02782257],\n",
       "       [0.64825581, 0.66117305, 0.68730803, 0.66797641, 0.59837464,\n",
       "        0.02440802],\n",
       "       [0.66182175, 0.66117305, 0.6765609 , 0.64685666, 0.57893629,\n",
       "        0.03144357],\n",
       "       [0.64341085, 0.6776539 , 0.68372569, 0.68516703, 0.61419665,\n",
       "        0.04400526],\n",
       "       [0.67877902, 0.69704314, 0.71903787, 0.69842833, 0.6264022 ,\n",
       "        0.04546845],\n",
       "       [0.69137592, 0.69122642, 0.7036848 , 0.67730848, 0.60696374,\n",
       "        0.03177292],\n",
       "       [0.66569772, 0.66941348, 0.6862846 , 0.67583495, 0.6056075 ,\n",
       "        0.03919891],\n",
       "       [0.65406982, 0.6572952 , 0.665302  , 0.63998039, 0.57260735,\n",
       "        0.05120333],\n",
       "       [0.64292631, 0.65341735, 0.68116684, 0.66306491, 0.59385423,\n",
       "        0.03397579],\n",
       "       [0.64147292, 0.64614639, 0.65813715, 0.63703343, 0.56989516,\n",
       "        0.05635362],\n",
       "       [0.63565891, 0.66262729, 0.665302  , 0.66895878, 0.5992789 ,\n",
       "        0.04077971],\n",
       "       [0.67587209, 0.68880271, 0.70777897, 0.6969548 , 0.625046  ,\n",
       "        0.0548714 ],\n",
       "       [0.68653106, 0.70382942, 0.71903787, 0.71660126, 0.64312846,\n",
       "        0.03461346],\n",
       "       [0.70300383, 0.73921474, 0.74411464, 0.73280952, 0.6580463 ,\n",
       "        0.04969664],\n",
       "       [0.71996119, 0.74600097, 0.76202661, 0.74852661, 0.67251211,\n",
       "        0.04766145],\n",
       "       [0.73982553, 0.74745521, 0.76867958, 0.73526526, 0.66030651,\n",
       "        0.05031056],\n",
       "       [0.76550388, 0.79059622, 0.80962134, 0.79666016, 0.71681365,\n",
       "        0.10120858],\n",
       "       [0.74854651, 0.76732913, 0.7840328 , 0.7804519 , 0.71911682,\n",
       "        0.06567045],\n",
       "       [0.75823643, 0.79301983, 0.80501535, 0.78831045, 0.72648688,\n",
       "        0.04828195],\n",
       "       [0.78924419, 0.79447407, 0.80706238, 0.77455795, 0.71358928,\n",
       "        0.06152981],\n",
       "       [0.76598837, 0.78041692, 0.80348004, 0.79223972, 0.73017189,\n",
       "        0.04455508],\n",
       "       [0.78488372, 0.79835191, 0.82702155, 0.80648339, 0.74353023,\n",
       "        0.03775975],\n",
       "       [0.80184109, 0.80222976, 0.82395082, 0.79027514, 0.72832946,\n",
       "        0.03492235],\n",
       "       [0.77761628, 0.78768783, 0.81729785, 0.7907662 , 0.72879006,\n",
       "        0.03271233],\n",
       "       [0.77325581, 0.78138628, 0.79785051, 0.77406679, 0.71312854,\n",
       "        0.0315204 ],\n",
       "       [0.7562984 , 0.75521086, 0.78096208, 0.75098236, 0.69147899,\n",
       "        0.03087142],\n",
       "       [0.74273261, 0.74697043, 0.77430911, 0.75392926, 0.69424286,\n",
       "        0.04384244],\n",
       "       [0.74127907, 0.74503146, 0.77840328, 0.75491163, 0.6951641 ,\n",
       "        0.03128876],\n",
       "       [0.74224806, 0.76635967, 0.78505624, 0.76375249, 0.7034554 ,\n",
       "        0.03586405]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 90\n",
    "\n",
    "normalized_train_data[i - 90:90, 0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(90, 1242):\n",
    "    X.append(normalized_train_data[i - 90:i, 0:6])\n",
    "    y.append(normalized_train_data[i, 0])\n",
    "\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 90, 6), (1152,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\GitHub\\ai-experiments\\ai-exp\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m42,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences=True, input_shape = (X.shape[1], X.shape[2])))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(Dense(units = 1, activation = 'linear'))\n",
    "\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', min_delta=1e-10, patience=10, verbose=True) # interrupt trainig if error or loss function don't improve\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1) # reduce learning rate after 5 epochs if loss does not improve\n",
    "mcp = ModelCheckpoint(filepath='../data/petr/weights.keras', monitor='loss', save_best_only=True, verbose=1) # when best results are found weights will be saved \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0540 - mean_absolute_error: 0.1684\n",
      "Epoch 1: loss improved from inf to 0.02452, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - loss: 0.0532 - mean_absolute_error: 0.1669 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0088 - mean_absolute_error: 0.0732\n",
      "Epoch 2: loss improved from 0.02452 to 0.00908, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0088 - mean_absolute_error: 0.0732 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0086 - mean_absolute_error: 0.0700\n",
      "Epoch 3: loss improved from 0.00908 to 0.00780, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0085 - mean_absolute_error: 0.0700 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0066 - mean_absolute_error: 0.0618\n",
      "Epoch 4: loss improved from 0.00780 to 0.00659, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 0.0066 - mean_absolute_error: 0.0618 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0063 - mean_absolute_error: 0.0614\n",
      "Epoch 5: loss did not improve from 0.00659\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 0.0063 - mean_absolute_error: 0.0614 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0065 - mean_absolute_error: 0.0632\n",
      "Epoch 6: loss did not improve from 0.00659\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 0.0065 - mean_absolute_error: 0.0632 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0063 - mean_absolute_error: 0.0598\n",
      "Epoch 7: loss improved from 0.00659 to 0.00609, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0063 - mean_absolute_error: 0.0598 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0056 - mean_absolute_error: 0.0570\n",
      "Epoch 8: loss improved from 0.00609 to 0.00554, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0056 - mean_absolute_error: 0.0570 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0060 - mean_absolute_error: 0.0584\n",
      "Epoch 9: loss did not improve from 0.00554\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.0059 - mean_absolute_error: 0.0584 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0063 - mean_absolute_error: 0.0605\n",
      "Epoch 10: loss did not improve from 0.00554\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0063 - mean_absolute_error: 0.0605 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0056 - mean_absolute_error: 0.0556\n",
      "Epoch 11: loss improved from 0.00554 to 0.00540, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 0.0056 - mean_absolute_error: 0.0556 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0047 - mean_absolute_error: 0.0517\n",
      "Epoch 12: loss improved from 0.00540 to 0.00509, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0047 - mean_absolute_error: 0.0518 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0052 - mean_absolute_error: 0.0556\n",
      "Epoch 13: loss did not improve from 0.00509\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0052 - mean_absolute_error: 0.0555 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0048 - mean_absolute_error: 0.0530\n",
      "Epoch 14: loss improved from 0.00509 to 0.00474, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - loss: 0.0048 - mean_absolute_error: 0.0530 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0053 - mean_absolute_error: 0.0557\n",
      "Epoch 15: loss did not improve from 0.00474\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0053 - mean_absolute_error: 0.0557 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0047 - mean_absolute_error: 0.0521\n",
      "Epoch 16: loss improved from 0.00474 to 0.00434, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - loss: 0.0047 - mean_absolute_error: 0.0520 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0049 - mean_absolute_error: 0.0524\n",
      "Epoch 17: loss did not improve from 0.00434\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0049 - mean_absolute_error: 0.0524 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0040 - mean_absolute_error: 0.0484\n",
      "Epoch 18: loss improved from 0.00434 to 0.00418, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0040 - mean_absolute_error: 0.0485 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0044 - mean_absolute_error: 0.0502\n",
      "Epoch 19: loss improved from 0.00418 to 0.00388, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0044 - mean_absolute_error: 0.0502 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0040 - mean_absolute_error: 0.0493\n",
      "Epoch 20: loss did not improve from 0.00388\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 0.0040 - mean_absolute_error: 0.0493 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0037 - mean_absolute_error: 0.0472\n",
      "Epoch 21: loss improved from 0.00388 to 0.00370, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0037 - mean_absolute_error: 0.0471 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0038 - mean_absolute_error: 0.0477\n",
      "Epoch 22: loss did not improve from 0.00370\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0038 - mean_absolute_error: 0.0477 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0039 - mean_absolute_error: 0.0483\n",
      "Epoch 23: loss did not improve from 0.00370\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - loss: 0.0039 - mean_absolute_error: 0.0483 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0037 - mean_absolute_error: 0.0463\n",
      "Epoch 24: loss improved from 0.00370 to 0.00350, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0037 - mean_absolute_error: 0.0463 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0034 - mean_absolute_error: 0.0450\n",
      "Epoch 25: loss did not improve from 0.00350\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0034 - mean_absolute_error: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0029 - mean_absolute_error: 0.0413\n",
      "Epoch 26: loss improved from 0.00350 to 0.00324, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0029 - mean_absolute_error: 0.0414 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0035 - mean_absolute_error: 0.0446\n",
      "Epoch 27: loss did not improve from 0.00324\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.0035 - mean_absolute_error: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0035 - mean_absolute_error: 0.0450\n",
      "Epoch 28: loss did not improve from 0.00324\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0035 - mean_absolute_error: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0032 - mean_absolute_error: 0.0431\n",
      "Epoch 29: loss improved from 0.00324 to 0.00314, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0032 - mean_absolute_error: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0030 - mean_absolute_error: 0.0420\n",
      "Epoch 30: loss improved from 0.00314 to 0.00285, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0030 - mean_absolute_error: 0.0420 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0025 - mean_absolute_error: 0.0391\n",
      "Epoch 31: loss did not improve from 0.00285\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0025 - mean_absolute_error: 0.0391 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0029 - mean_absolute_error: 0.0406\n",
      "Epoch 32: loss did not improve from 0.00285\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0029 - mean_absolute_error: 0.0406 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0035 - mean_absolute_error: 0.0435\n",
      "Epoch 33: loss did not improve from 0.00285\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0035 - mean_absolute_error: 0.0435 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0030 - mean_absolute_error: 0.0418\n",
      "Epoch 34: loss improved from 0.00285 to 0.00280, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0030 - mean_absolute_error: 0.0417 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0029 - mean_absolute_error: 0.0391\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 35: loss did not improve from 0.00280\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0029 - mean_absolute_error: 0.0391 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0026 - mean_absolute_error: 0.0392\n",
      "Epoch 36: loss improved from 0.00280 to 0.00266, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0026 - mean_absolute_error: 0.0392 - learning_rate: 2.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0025 - mean_absolute_error: 0.0373\n",
      "Epoch 37: loss improved from 0.00266 to 0.00238, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0025 - mean_absolute_error: 0.0373 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0025 - mean_absolute_error: 0.0380\n",
      "Epoch 38: loss did not improve from 0.00238\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0025 - mean_absolute_error: 0.0380 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0023 - mean_absolute_error: 0.0349\n",
      "Epoch 39: loss did not improve from 0.00238\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0023 - mean_absolute_error: 0.0349 - learning_rate: 2.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0024 - mean_absolute_error: 0.0374\n",
      "Epoch 40: loss improved from 0.00238 to 0.00236, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0024 - mean_absolute_error: 0.0374 - learning_rate: 2.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0023 - mean_absolute_error: 0.0364\n",
      "Epoch 41: loss did not improve from 0.00236\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0023 - mean_absolute_error: 0.0364 - learning_rate: 2.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0022 - mean_absolute_error: 0.0366\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 42: loss improved from 0.00236 to 0.00232, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0022 - mean_absolute_error: 0.0365 - learning_rate: 2.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0024 - mean_absolute_error: 0.0365\n",
      "Epoch 43: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0024 - mean_absolute_error: 0.0365 - learning_rate: 4.0000e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0030 - mean_absolute_error: 0.0389\n",
      "Epoch 44: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0029 - mean_absolute_error: 0.0388 - learning_rate: 4.0000e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0023 - mean_absolute_error: 0.0369\n",
      "Epoch 45: loss improved from 0.00232 to 0.00229, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0023 - mean_absolute_error: 0.0369 - learning_rate: 4.0000e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0026 - mean_absolute_error: 0.0379\n",
      "Epoch 46: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - loss: 0.0026 - mean_absolute_error: 0.0378 - learning_rate: 4.0000e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0026 - mean_absolute_error: 0.0388\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 47: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 0.0026 - mean_absolute_error: 0.0387 - learning_rate: 4.0000e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0022 - mean_absolute_error: 0.0358\n",
      "Epoch 48: loss improved from 0.00229 to 0.00222, saving model to ../data/petr/weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0022 - mean_absolute_error: 0.0358 - learning_rate: 8.0000e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0026 - mean_absolute_error: 0.0373\n",
      "Epoch 49: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 0.0026 - mean_absolute_error: 0.0373 - learning_rate: 8.0000e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0022 - mean_absolute_error: 0.0361\n",
      "Epoch 50: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0022 - mean_absolute_error: 0.0361 - learning_rate: 8.0000e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0025 - mean_absolute_error: 0.0374\n",
      "Epoch 51: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0025 - mean_absolute_error: 0.0374 - learning_rate: 8.0000e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0023 - mean_absolute_error: 0.0364\n",
      "Epoch 52: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0023 - mean_absolute_error: 0.0364 - learning_rate: 8.0000e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0025 - mean_absolute_error: 0.0384\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 53: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 0.0025 - mean_absolute_error: 0.0383 - learning_rate: 8.0000e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0026 - mean_absolute_error: 0.0381\n",
      "Epoch 54: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 0.0026 - mean_absolute_error: 0.0381 - learning_rate: 1.6000e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0025 - mean_absolute_error: 0.0374\n",
      "Epoch 55: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - loss: 0.0025 - mean_absolute_error: 0.0374 - learning_rate: 1.6000e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0025 - mean_absolute_error: 0.0370\n",
      "Epoch 56: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 0.0025 - mean_absolute_error: 0.0370 - learning_rate: 1.6000e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0025 - mean_absolute_error: 0.0385\n",
      "Epoch 57: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0025 - mean_absolute_error: 0.0384 - learning_rate: 1.6000e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0022 - mean_absolute_error: 0.0365\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 58: loss did not improve from 0.00222\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 0.0022 - mean_absolute_error: 0.0365 - learning_rate: 1.6000e-06\n",
      "Epoch 58: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x255e0c11e20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X, y, epochs=100, batch_size=32, callbacks=[es, rlr, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.516966</td>\n",
       "      <td>33461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.719999</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.666668</td>\n",
       "      <td>55940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.696608</td>\n",
       "      <td>37064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>16.830000</td>\n",
       "      <td>16.796408</td>\n",
       "      <td>26958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>28400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>35070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>16.766466</td>\n",
       "      <td>28547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>16.879999</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>16.840000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.215569</td>\n",
       "      <td>37921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>17.040001</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.265469</td>\n",
       "      <td>45912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.315371</td>\n",
       "      <td>28945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.840000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>17.614771</td>\n",
       "      <td>58618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>17.920000</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>17.809999</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>18.323355</td>\n",
       "      <td>58488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>18.530001</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>18.183632</td>\n",
       "      <td>48575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>18.030001</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.223553</td>\n",
       "      <td>33470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.090000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.433134</td>\n",
       "      <td>33920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.459999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.240000</td>\n",
       "      <td>18.203592</td>\n",
       "      <td>35567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>89768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       "1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       "2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       "3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       "4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       "5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       "6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       "7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       "8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       "9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       "10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       "11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       "12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       "13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       "14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       "15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       "16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "      Volume  \n",
       "0   33461800  \n",
       "1   55940900  \n",
       "2   37064900  \n",
       "3   26958200  \n",
       "4   28400000  \n",
       "5   35070900  \n",
       "6   28547700  \n",
       "7   37921500  \n",
       "8   45912100  \n",
       "9   28945400  \n",
       "10  58618300  \n",
       "11  58488900  \n",
       "12  48575800  \n",
       "13  33470200  \n",
       "14  33920000  \n",
       "15  35567700  \n",
       "16  89768200  \n",
       "17         0  \n",
       "18  81989500  \n",
       "19  55726200  \n",
       "20  46203000  \n",
       "21  41576600  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../data/petr/petr4_teste.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_data.iloc[:, 1:2].values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       " 1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       " 2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       " 3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       " 4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       " ...          ...        ...        ...        ...        ...        ...   \n",
       " 1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       " 1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       " 1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       " 1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       " 1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       " \n",
       "           Volume  \n",
       " 0     30182600.0  \n",
       " 1     30552600.0  \n",
       " 2     36141000.0  \n",
       " 3     28069600.0  \n",
       " 4     29091300.0  \n",
       " ...          ...  \n",
       " 1240         0.0  \n",
       " 1241  22173100.0  \n",
       " 1242  23552200.0  \n",
       " 1243  19011500.0  \n",
       " 1244         0.0  \n",
       " \n",
       " [1242 rows x 7 columns],\n",
       "           Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       " 1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       " 2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       " 3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       " 4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       " 5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       " 6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       " 7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       " 8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       " 9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       " 10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       " 11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       " 12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       " 13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       " 14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       " 15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       " 16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       " 17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       " 18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       " 19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       " 20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       " 21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       " \n",
       "       Volume  \n",
       " 0   33461800  \n",
       " 1   55940900  \n",
       " 2   37064900  \n",
       " 3   26958200  \n",
       " 4   28400000  \n",
       " 5   35070900  \n",
       " 6   28547700  \n",
       " 7   37921500  \n",
       " 8   45912100  \n",
       " 9   28945400  \n",
       " 10  58618300  \n",
       " 11  58488900  \n",
       " 12  48575800  \n",
       " 13  33470200  \n",
       " 14  33920000  \n",
       " 15  35567700  \n",
       " 16  89768200  \n",
       " 17         0  \n",
       " 18  81989500  \n",
       " 19  55726200  \n",
       " 20  46203000  \n",
       " 21  41576600  ]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [data, test_data]\n",
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1   2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2   2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3   2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4   2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "        Volume  \n",
       "0   30182600.0  \n",
       "1   30552600.0  \n",
       "2   36141000.0  \n",
       "3   28069600.0  \n",
       "4   29091300.0  \n",
       "..         ...  \n",
       "17         0.0  \n",
       "18  81989500.0  \n",
       "19  55726200.0  \n",
       "20  46203000.0  \n",
       "21  41576600.0  \n",
       "\n",
       "[1264 rows x 7 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data = pd.concat(frames)\n",
    "complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = complete_data.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3930000e+01, 1.4030000e+01, 1.3760000e+01, 1.3870000e+01,\n",
       "        1.3842316e+01, 2.7208100e+07],\n",
       "       [1.3760000e+01, 1.3850000e+01, 1.3680000e+01, 1.3850000e+01,\n",
       "        1.3822356e+01, 2.7306400e+07],\n",
       "       [1.3790000e+01, 1.3900000e+01, 1.3440000e+01, 1.3450000e+01,\n",
       "        1.3423154e+01, 5.8871700e+07],\n",
       "       [1.3530000e+01, 1.3770000e+01, 1.3470000e+01, 1.3650000e+01,\n",
       "        1.3622754e+01, 8.2909400e+07],\n",
       "       [1.3850000e+01, 1.4190000e+01, 1.3820000e+01, 1.4020000e+01,\n",
       "        1.3992017e+01, 6.0260300e+07],\n",
       "       [1.3960000e+01, 1.4180000e+01, 1.3940000e+01, 1.4170000e+01,\n",
       "        1.4141717e+01, 1.8139300e+07],\n",
       "       [1.4570000e+01, 1.4650000e+01, 1.4230000e+01, 1.4410000e+01,\n",
       "        1.4381238e+01, 5.6476800e+07],\n",
       "       [1.4650000e+01, 1.5020000e+01, 1.4510000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 6.8418200e+07],\n",
       "       [1.5020000e+01, 1.5020000e+01, 1.5020000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 0.0000000e+00],\n",
       "       [1.5100000e+01, 1.5150000e+01, 1.4690000e+01, 1.4710000e+01,\n",
       "        1.4680639e+01, 3.6337400e+07],\n",
       "       [1.4880000e+01, 1.5050000e+01, 1.4810000e+01, 1.4990000e+01,\n",
       "        1.4960080e+01, 3.4915900e+07],\n",
       "       [1.4980000e+01, 1.5160000e+01, 1.4860000e+01, 1.4870000e+01,\n",
       "        1.4840320e+01, 4.9702800e+07],\n",
       "       [1.4940000e+01, 1.5100000e+01, 1.4810000e+01, 1.5030000e+01,\n",
       "        1.5000000e+01, 3.7010200e+07],\n",
       "       [1.5030000e+01, 1.5260000e+01, 1.5020000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 3.4413800e+07],\n",
       "       [1.5070000e+01, 1.5170000e+01, 1.4990000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7784700e+07],\n",
       "       [1.5020000e+01, 1.5190000e+01, 1.4980000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7601200e+07],\n",
       "       [1.5100000e+01, 1.5170000e+01, 1.4920000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 3.5822100e+07],\n",
       "       [1.5250000e+01, 1.5880000e+01, 1.5070000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 8.0267000e+07],\n",
       "       [1.5850000e+01, 1.5960000e+01, 1.5580000e+01, 1.5670000e+01,\n",
       "        1.5638723e+01, 4.6258800e+07],\n",
       "       [1.5600000e+01, 1.5800000e+01, 1.5430000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.0928300e+07],\n",
       "       [1.5790000e+01, 1.5960000e+01, 1.5700000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.6733200e+07],\n",
       "       [1.5860000e+01, 1.5900000e+01, 1.5560000e+01, 1.5560000e+01,\n",
       "        1.5528943e+01, 3.7874200e+07],\n",
       "       [1.5700000e+01, 1.5720000e+01, 1.5110000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 4.1819300e+07],\n",
       "       [1.5370000e+01, 1.5500000e+01, 1.5220000e+01, 1.5340000e+01,\n",
       "        1.5309381e+01, 3.3829000e+07],\n",
       "       [1.5500000e+01, 1.5520000e+01, 1.5300000e+01, 1.5300000e+01,\n",
       "        1.5269462e+01, 2.8638300e+07],\n",
       "       [1.5190000e+01, 1.5400000e+01, 1.5060000e+01, 1.5400000e+01,\n",
       "        1.5369262e+01, 2.9826200e+07],\n",
       "       [1.5600000e+01, 1.5980000e+01, 1.5520000e+01, 1.5980000e+01,\n",
       "        1.5948104e+01, 5.0636700e+07],\n",
       "       [1.5900000e+01, 1.5940000e+01, 1.5650000e+01, 1.5660000e+01,\n",
       "        1.5628743e+01, 4.7798600e+07],\n",
       "       [1.5880000e+01, 1.6110001e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 5.5361300e+07],\n",
       "       [1.5660000e+01, 1.5770000e+01, 1.5540000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.1741300e+07],\n",
       "       [1.5610000e+01, 1.5890000e+01, 1.5590000e+01, 1.5890000e+01,\n",
       "        1.5858284e+01, 2.7904700e+07],\n",
       "       [1.6129999e+01, 1.6190001e+01, 1.6010000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 4.7066600e+07],\n",
       "       [1.6170000e+01, 1.6250000e+01, 1.6010000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 4.0422100e+07],\n",
       "       [1.6080000e+01, 1.6080000e+01, 1.6080000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 0.0000000e+00],\n",
       "       [1.6230000e+01, 1.6290001e+01, 1.6059999e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 2.4210000e+07],\n",
       "       [1.6160000e+01, 1.6260000e+01, 1.6000000e+01, 1.6120001e+01,\n",
       "        1.6087826e+01, 4.4699700e+07],\n",
       "       [1.6139999e+01, 1.6219999e+01, 1.6070000e+01, 1.6129999e+01,\n",
       "        1.6097803e+01, 2.5524800e+07],\n",
       "       [1.6219999e+01, 1.6280001e+01, 1.6129999e+01, 1.6160000e+01,\n",
       "        1.6127745e+01, 2.5706200e+07],\n",
       "       [1.6000000e+01, 1.6160000e+01, 1.5900000e+01, 1.6150000e+01,\n",
       "        1.6117765e+01, 2.4672800e+07],\n",
       "       [1.6190001e+01, 1.6389999e+01, 1.6170000e+01, 1.6219999e+01,\n",
       "        1.6187624e+01, 3.2417500e+07],\n",
       "       [1.6290001e+01, 1.6290001e+01, 1.6120001e+01, 1.6200001e+01,\n",
       "        1.6167665e+01, 2.9389900e+07],\n",
       "       [1.6290001e+01, 1.6510000e+01, 1.6120001e+01, 1.6510000e+01,\n",
       "        1.6477047e+01, 4.6249500e+07],\n",
       "       [1.6530001e+01, 1.6730000e+01, 1.6450001e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.7608200e+07],\n",
       "       [1.6780001e+01, 1.6889999e+01, 1.6660000e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7848300e+07],\n",
       "       [1.6770000e+01, 1.7090000e+01, 1.6650000e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 4.5640100e+07],\n",
       "       [1.6969999e+01, 1.7170000e+01, 1.6740000e+01, 1.6780001e+01,\n",
       "        1.6746508e+01, 5.5355600e+07],\n",
       "       [1.6900000e+01, 1.6950001e+01, 1.6719999e+01, 1.6770000e+01,\n",
       "        1.6736528e+01, 3.2249000e+07],\n",
       "       [1.6990000e+01, 1.7100000e+01, 1.6879999e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 3.8876600e+07],\n",
       "       [1.6900000e+01, 1.6900000e+01, 1.6900000e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 0.0000000e+00],\n",
       "       [1.6959999e+01, 1.7010000e+01, 1.6680000e+01, 1.6940001e+01,\n",
       "        1.6906189e+01, 3.2605400e+07],\n",
       "       [1.7049999e+01, 1.7440001e+01, 1.6980000e+01, 1.7430000e+01,\n",
       "        1.7395210e+01, 4.6056100e+07],\n",
       "       [1.7309999e+01, 1.7350000e+01, 1.6500000e+01, 1.6500000e+01,\n",
       "        1.6467066e+01, 6.1098400e+07],\n",
       "       [1.6690001e+01, 1.6950001e+01, 1.6510000e+01, 1.6950001e+01,\n",
       "        1.6916168e+01, 4.1179600e+07],\n",
       "       [1.6889999e+01, 1.6940001e+01, 1.6719999e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 2.9399400e+07],\n",
       "       [1.6709999e+01, 1.6809999e+01, 1.6510000e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.5959400e+07],\n",
       "       [1.6690001e+01, 1.6770000e+01, 1.6389999e+01, 1.6639999e+01,\n",
       "        1.6606787e+01, 2.8697700e+07],\n",
       "       [1.6639999e+01, 1.6639999e+01, 1.5280000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 8.8765600e+07],\n",
       "       [1.5350000e+01, 1.5350000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 0.0000000e+00],\n",
       "       [1.5620000e+01, 1.6040001e+01, 1.5480000e+01, 1.5810000e+01,\n",
       "        1.5778444e+01, 4.2703800e+07],\n",
       "       [1.5920000e+01, 1.6120001e+01, 1.5810000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 3.8376900e+07],\n",
       "       [1.6020000e+01, 1.6020000e+01, 1.6020000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 0.0000000e+00],\n",
       "       [1.6150000e+01, 1.6309999e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 4.5817800e+07],\n",
       "       [1.6090000e+01, 1.6240000e+01, 1.5930000e+01, 1.6110001e+01,\n",
       "        1.6077845e+01, 3.7444900e+07],\n",
       "       [1.5980000e+01, 1.6260000e+01, 1.5940000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 1.5403600e+07],\n",
       "       [1.6250000e+01, 1.6370001e+01, 1.6040001e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.8790700e+07],\n",
       "       [1.6010000e+01, 1.6020000e+01, 1.5780000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 2.8445800e+07],\n",
       "       [1.5930000e+01, 1.6040001e+01, 1.5810000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.0429600e+07],\n",
       "       [1.5870000e+01, 1.5920000e+01, 1.5320000e+01, 1.5330000e+01,\n",
       "        1.5299401e+01, 4.5973000e+07],\n",
       "       [1.5300000e+01, 1.5470000e+01, 1.4990000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 5.2811400e+07],\n",
       "       [1.5340000e+01, 1.5770000e+01, 1.5260000e+01, 1.5610000e+01,\n",
       "        1.5578842e+01, 4.2703800e+07],\n",
       "       [1.5650000e+01, 1.5800000e+01, 1.5460000e+01, 1.5480000e+01,\n",
       "        1.5449101e+01, 4.3821500e+07],\n",
       "       [1.5500000e+01, 1.5830000e+01, 1.5210000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 3.0228000e+07],\n",
       "       [1.5220000e+01, 1.5700000e+01, 1.5140000e+01, 1.5520000e+01,\n",
       "        1.5489023e+01, 3.9238500e+07],\n",
       "       [1.5300000e+01, 1.5490000e+01, 1.5070000e+01, 1.5260000e+01,\n",
       "        1.5229542e+01, 3.7281400e+07],\n",
       "       [1.5510000e+01, 1.5680000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 3.9584500e+07],\n",
       "       [1.5480000e+01, 1.5570000e+01, 1.5370000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 2.1281600e+07],\n",
       "       [1.5360000e+01, 1.5490000e+01, 1.5180000e+01, 1.5490000e+01,\n",
       "        1.5459082e+01, 3.6201200e+07],\n",
       "       [1.5650000e+01, 1.5680000e+01, 1.5110000e+01, 1.5180000e+01,\n",
       "        1.5149701e+01, 4.6828900e+07],\n",
       "       [1.5100000e+01, 1.5310000e+01, 1.5000000e+01, 1.5010000e+01,\n",
       "        1.4980041e+01, 3.7177300e+07],\n",
       "       [1.5050000e+01, 1.5240000e+01, 1.4950000e+01, 1.4950000e+01,\n",
       "        1.4920160e+01, 5.5668300e+07],\n",
       "       [1.5160000e+01, 1.5330000e+01, 1.5130000e+01, 1.5220000e+01,\n",
       "        1.5189621e+01, 4.2760400e+07],\n",
       "       [1.5180000e+01, 1.5250000e+01, 1.5060000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 2.2639700e+07],\n",
       "       [1.5210000e+01, 1.5300000e+01, 1.5170000e+01, 1.5240000e+01,\n",
       "        1.5209581e+01, 2.0149700e+07],\n",
       "       [1.5310000e+01, 1.5870000e+01, 1.5300000e+01, 1.5860000e+01,\n",
       "        1.5828343e+01, 4.7219400e+07],\n",
       "       [1.5750000e+01, 1.5890000e+01, 1.5690000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 1.8708500e+07],\n",
       "       [1.5750000e+01, 1.5750000e+01, 1.5750000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 0.0000000e+00],\n",
       "       [1.5750000e+01, 1.5990000e+01, 1.5690000e+01, 1.5970000e+01,\n",
       "        1.5938125e+01, 2.2173100e+07],\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00],\n",
       "       [1.6190001e+01, 1.6549999e+01, 1.6190001e+01, 1.6549999e+01,\n",
       "        1.6516966e+01, 3.3461800e+07],\n",
       "       [1.6490000e+01, 1.6719999e+01, 1.6370001e+01, 1.6700001e+01,\n",
       "        1.6666668e+01, 5.5940900e+07],\n",
       "       [1.6780001e+01, 1.6959999e+01, 1.6620001e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7064900e+07],\n",
       "       [1.6700001e+01, 1.6860001e+01, 1.6570000e+01, 1.6830000e+01,\n",
       "        1.6796408e+01, 2.6958200e+07],\n",
       "       [1.6740000e+01, 1.7030001e+01, 1.6709999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 2.8400000e+07],\n",
       "       [1.7030001e+01, 1.7160000e+01, 1.6959999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 3.5070900e+07],\n",
       "       [1.6920000e+01, 1.7049999e+01, 1.6770000e+01, 1.6799999e+01,\n",
       "        1.6766466e+01, 2.8547700e+07],\n",
       "       [1.6879999e+01, 1.7299999e+01, 1.6840000e+01, 1.7250000e+01,\n",
       "        1.7215569e+01, 3.7921500e+07],\n",
       "       [1.7040001e+01, 1.7410000e+01, 1.7020000e+01, 1.7299999e+01,\n",
       "        1.7265469e+01, 4.5912100e+07],\n",
       "       [1.7320000e+01, 1.7440001e+01, 1.7150000e+01, 1.7350000e+01,\n",
       "        1.7315371e+01, 2.8945400e+07],\n",
       "       [1.7350000e+01, 1.7840000e+01, 1.7299999e+01, 1.7650000e+01,\n",
       "        1.7614771e+01, 5.8618300e+07],\n",
       "       [1.7920000e+01, 1.8360001e+01, 1.7809999e+01, 1.8360001e+01,\n",
       "        1.8323355e+01, 5.8488900e+07],\n",
       "       [1.8350000e+01, 1.8530001e+01, 1.7930000e+01, 1.8219999e+01,\n",
       "        1.8183632e+01, 4.8575800e+07],\n",
       "       [1.8309999e+01, 1.8420000e+01, 1.8030001e+01, 1.8260000e+01,\n",
       "        1.8223553e+01, 3.3470200e+07],\n",
       "       [1.8260000e+01, 1.8469999e+01, 1.8090000e+01, 1.8469999e+01,\n",
       "        1.8433134e+01, 3.3920000e+07],\n",
       "       [1.8400000e+01, 1.8459999e+01, 1.8000000e+01, 1.8240000e+01,\n",
       "        1.8203592e+01, 3.5567700e+07],\n",
       "       [1.8420000e+01, 1.9629999e+01, 1.8420000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 8.9768200e+07],\n",
       "       [1.9340000e+01, 1.9340000e+01, 1.9340000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 0.0000000e+00],\n",
       "       [1.9620001e+01, 1.9980000e+01, 1.9100000e+01, 1.9930000e+01,\n",
       "        1.9890221e+01, 8.1989500e+07],\n",
       "       [1.9670000e+01, 2.0049999e+01, 1.9570000e+01, 1.9850000e+01,\n",
       "        1.9810381e+01, 5.5726200e+07],\n",
       "       [1.9770000e+01, 1.9770000e+01, 1.9360001e+01, 1.9490000e+01,\n",
       "        1.9451097e+01, 4.6203000e+07],\n",
       "       [1.9740000e+01, 1.9930000e+01, 1.9680000e+01, 1.9700001e+01,\n",
       "        1.9660681e+01, 4.1576600e+07]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = complete_data[len(complete_data)-len(test_data)-90 : ].values\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = normalizer.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 6)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(90, len(inputs)):\n",
    "    X_test.append(inputs[i-90:i,:])\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 6)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i should check if I really can do that without any prejudice\n",
    "prediction = normalizer_prediction.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.113886],\n",
       "       [16.220993],\n",
       "       [16.32456 ],\n",
       "       [16.442448],\n",
       "       [16.56965 ],\n",
       "       [16.69863 ],\n",
       "       [16.823517],\n",
       "       [16.920347],\n",
       "       [16.99123 ],\n",
       "       [17.049522],\n",
       "       [17.108595],\n",
       "       [17.19123 ],\n",
       "       [17.342505],\n",
       "       [17.565956],\n",
       "       [17.818779],\n",
       "       [18.050087],\n",
       "       [18.211266],\n",
       "       [18.346195],\n",
       "       [18.49354 ],\n",
       "       [18.703808],\n",
       "       [18.969582],\n",
       "       [19.21944 ]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.41708, 17.87454563636364)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46113338346724075"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, color='red', label='Real price')\n",
    "plt.plot(prediction, color='blue', label='Prediction')\n",
    "plt.title('Stock prices prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
